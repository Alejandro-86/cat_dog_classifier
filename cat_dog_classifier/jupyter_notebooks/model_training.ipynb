{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model.\n",
    "This notebook will be used to train and evaluate the model. We will do the following steps:\n",
    "\n",
    "1. Load and transform the data\n",
    "2. Load the data in batches using a custom data generator (Dataloader)\n",
    "3. Define train and test functions\n",
    "4. Define the model architecture and train loop while visualizing the loss and accuracy of the model.\n",
    "6. Evaluate the trained model on the test set\n",
    "7. Save the trained model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and transform the data\n",
    "\n",
    "In this step we will load the data and split it into training, validation and testing sets.\n",
    "\n",
    "We will split the training set into 80% training and 20% validation sets and and use the test set to evaluate the model after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import  Resize, CenterCrop, ToTensor, Compose\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "trans = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset from disk\n",
    "data_dir = \"../dataset\"\n",
    "train_dir = os.path.join(data_dir, \"training_set\")\n",
    "test_dir = os.path.join(data_dir, \"test_set\")\n",
    "training_set = ImageFolder(train_dir, transform=trans)\n",
    "test_set = ImageFolder(test_dir, transform=trans)\n",
    "\n",
    "# Split training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(training_set))  # 80% for training\n",
    "val_size = len(training_set) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(training_set, [train_size, val_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data in batches using a custom data generator (Dataloader)\n",
    "\n",
    "We create a custom data generator to load the data in batches. This is done to avoid loading the entire dataset into memory at once. We will use the data generator to load the training, validation and testing data and sending the batch to the GPU (if available) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "from torch.backends.mps import is_available as mps_is_available\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda import is_available\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if is_available()\n",
    "    else \"mps\"\n",
    "    if mps_is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class CustomDataLoader(DataLoader):\n",
    "    def __iter__(self):\n",
    "        for batch in tqdm(super(CustomDataLoader, self).__iter__(), unit='batch', dynamic_ncols=True):\n",
    "            yield [item.to(device) if isinstance(item, torch.Tensor) else item for item in batch]\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "\n",
    "params = {\"batch_size\": 64, \"shuffle\": True}\n",
    "train_loader = CustomDataLoader(train_dataset, **params)\n",
    "val_loader = CustomDataLoader(val_dataset, **params)\n",
    "test_loader = CustomDataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Define train and test functions\n",
    "\n",
    "We define the train and test functions to train and evaluate the model. The train function will be used to train the model on the training set and the test function will be used to evaluate the model on the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import float as torch_float\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch % 100 == 0 and batch > 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            writer.add_scalar(\"Loss/train\", loss, current)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch_float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the model architecture and train the model\n",
    "\n",
    "We define the model architecture and train the model on the training set. We will use the SGD optimizer and the Cross Entropy Loss function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:36<00:00,  1.83batch/s]\n",
      "100%|██████████| 17/17 [00:05<00:00,  2.90batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.534390\n",
      "-------------------------------\n",
      "Epoch 2:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:35<00:00,  1.88batch/s]\n",
      "100%|██████████| 17/17 [00:05<00:00,  2.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.500273\n",
      "-------------------------------\n",
      "Epoch 3:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:35<00:00,  1.89batch/s]\n",
      "100%|██████████| 17/17 [00:05<00:00,  2.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.487948\n",
      "-------------------------------\n",
      "Epoch 4:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [23:04<00:00, 20.66s/batch] \n",
      "100%|██████████| 17/17 [00:09<00:00,  1.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.479376\n",
      "-------------------------------\n",
      "Epoch 5:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:33<00:00,  1.98batch/s]\n",
      "100%|██████████| 17/17 [00:07<00:00,  2.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.491938\n",
      "-------------------------------\n",
      "Training complete! \n",
      " ------------------------------- \n",
      "Computing test error...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.656415\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "# Load pretrained ResNet-18 model\n",
    "model = resnet18().to(device)\n",
    "\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "# Loss functions specified in the torch.nn package\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}:  \")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(val_loader, model, loss_fn)\n",
    "    print(\"-------------------------------\")\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "\n",
    "print(\"Training complete! \\n ------------------------------- \\nComputing test error...\")\n",
    "test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
